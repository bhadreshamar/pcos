import streamlit as st

st.write ('PCOS')

st.info ('This app builds a machine learning model to detect PCOS')

# -*- coding: utf-8 -*-
"""BARCA_PCOS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15gaIo16UzxmPveYtDObPxMA2PWwS64O5

Pt.1
Using symptoms to give a risk prediction on wheather a patient should visit a physician
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

data=pd.read_csv('/content/drive/MyDrive/Aarnav/PCOS_data_infertility.csv')

data.head()

data.columns

data.info()

print(data['PCOS (Y/N)'].value_counts())

print(data[' Age (yrs)'].value_counts())

print(data['Weight (Kg)'].value_counts())

print(data['Height(Cm) '].value_counts())

print(data['BMI'].value_counts())

print(data['Blood Group'].value_counts())
if

print(data['Pulse rate(bpm) '].value_counts())

print(data['RR (breaths/min)'].value_counts())

print(data['Hb(g/dl)'].value_counts())

print(data['Cycle(R/I)'].value_counts())

print(data['Cycle length(days)'].value_counts())

print(data['Marraige Status (Yrs)'].value_counts())

print(data['Pregnant(Y/N)'].value_counts())

print(data['No. of aborptions'].value_counts())

print(data['  I   beta-HCG(mIU/mL)'].value_counts())

print(data['II    beta-HCG(mIU/mL)'].value_counts())

print(data['FSH(mIU/mL)'].value_counts())

print(data['LH(mIU/mL)'].value_counts())

print(data['FSH/LH'].value_counts())

print(data['Hip(inch)'].value_counts())

print(data['Waist(inch)'].value_counts())

print(data['Waist:Hip Ratio'].value_counts())

print(data['TSH (mIU/L)'].value_counts())

print(data['TSH (mIU/L)'].value_counts())

print(data['AMH(ng/mL)'].value_counts())

print(data['PRL(ng/mL)'].value_counts())

print(data['Vit D3 (ng/mL)'].value_counts())

print(data['PRG(ng/mL)'].value_counts())

print(data['RBS(mg/dl)'].value_counts())

print(data['Weight gain(Y/N)'].value_counts())

print(data['hair growth(Y/N)'].value_counts())

print(data['Skin darkening (Y/N)'].value_counts())

print(data['Hair loss(Y/N)'].value_counts())

print(data['Pimples(Y/N)'].value_counts())

print(data['Fast food (Y/N)'].value_counts())

print(data['Reg.Exercise(Y/N)'].value_counts())

print(data['BP _Systolic (mmHg)'].value_counts())

print(data['BP _Diastolic (mmHg)'].value_counts())

print(data['Follicle No. (L)'].value_counts())

print(data['Follicle No. (R)'].value_counts())

print(data['Avg. F size (L) (mm)'].value_counts())

print(data['Avg. F size (R) (mm)'].value_counts())

print(data['Endometrium (mm)'].value_counts())

# prompt: Create a heat map of all the symptoms listed above

import matplotlib.pyplot as plt

# Select columns to keep for the heatmap
columns_to_keep = [
    'PCOS (Y/N)', ' Age (yrs)', 'Weight (Kg)', 'Height(Cm) ', 'BMI', 'Blood Group', 'Pulse rate(bpm) ',
    'RR (breaths/min)', 'Hb(g/dl)', 'Cycle(R/I)', 'Cycle length(days)', 'Marraige Status (Yrs)',
    'Pregnant(Y/N)', 'No. of aborptions', '  I   beta-HCG(mIU/mL)', 'II    beta-HCG(mIU/mL)',
    'FSH(mIU/mL)', 'LH(mIU/mL)', 'FSH/LH', 'Hip(inch)', 'Waist(inch)', 'Waist:Hip Ratio',
    'TSH (mIU/L)', 'AMH(ng/mL)', 'PRL(ng/mL)', 'Vit D3 (ng/mL)', 'PRG(ng/mL)', 'RBS(mg/dl)',
    'Weight gain(Y/N)', 'hair growth(Y/N)', 'Skin darkening (Y/N)', 'Hair loss(Y/N)',
    'Pimples(Y/N)', 'Fast food (Y/N)', 'Reg.Exercise(Y/N)'
]
data_heatmap = data[columns_to_keep].copy()


# Convert relevant columns to numeric, coercing errors to NaN
data_heatmap['II    beta-HCG(mIU/mL)'] = pd.to_numeric(data_heatmap['II    beta-HCG(mIU/mL)'], errors='coerce')
data_heatmap['AMH(ng/mL)'] = pd.to_numeric(data_heatmap['AMH(ng/mL)'], errors='coerce')


# Calculate the correlation matrix
correlation_matrix = data_heatmap.corr()

# Create the heatmap
plt.figure(figsize=(16, 12))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Heatmap of Symptom Correlations')
plt.show()

data=pd.read_csv('/content/drive/MyDrive/Aarnav/PCOS_data_infertility.csv')
data.columns = data.columns.str.strip()
X = data.drop(["PCOS (Y/N)"], axis=1)
y = data["PCOS (Y/N)"]

"""# Task
Explain the error in the selected code, fix it if possible, and incorporate the changes into the existing code. If not, diagnose the error.

## Handle missing values

### Subtask:
Identify and address any missing values in the dataset.

**Reasoning**:
Check for missing values in the `data` DataFrame.
"""

print(data.isnull().sum())

"""**Reasoning**:
Handle the missing values in the identified columns by dropping the 'Unnamed: 44' column due to a large number of missing values and imputing the missing values in 'Marraige Status (Yrs)' and 'Fast food (Y/N)' with the mode.


"""

data = data.drop('Unnamed: 44', axis=1)
data['Marraige Status (Yrs)'].fillna(data['Marraige Status (Yrs)').mode()[0], inplace=True)
data['Fast food (Y/N)'].fillna(data['Fast food (Y/N)').mode()[0], inplace=True)
print(data.isnull().sum())

"""**Reasoning**:
Correct the syntax error in the previous code block and re-run the code to handle the missing values and verify the result.


"""

data = data.drop('Unnamed: 44', axis=1)
data['Marraige Status (Yrs)'].fillna(data['Marraige Status (Yrs)'].mode()[0], inplace=True)
data['Fast food (Y/N)'].fillna(data['Fast food (Y/N)'].mode()[0], inplace=True)
print(data.isnull().sum())

"""## Encode categorical features

### Subtask:
Convert categorical variables into a numerical format suitable for machine learning models.

**Reasoning**:
Identify and one-hot encode the categorical columns in the `X` DataFrame, then drop the original columns.
"""

categorical_cols = X.select_dtypes(include=['object']).columns
X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)
display(X.head())

"""## Scale numerical features

### Subtask:
Standardize or normalize numerical features to ensure they have a similar scale.

**Reasoning**:
Identify numerical columns, instantiate StandardScaler, and apply scaling to the numerical columns of X.
"""

from sklearn.preprocessing import StandardScaler

numerical_cols = X.select_dtypes(include=np.number).columns
scaler = StandardScaler()
X[numerical_cols] = scaler.fit_transform(X[numerical_cols])

# Impute missing values after scaling but before splitting
X['Marraige Status (Yrs)'].fillna(X['Marraige Status (Yrs)'].mode()[0], inplace=True)
X['Fast food (Y/N)'].fillna(X['Fast food (Y/N)'].mode()[0], inplace=True)

display(X.head())

"""## Split data

### Subtask:
Divide the dataset into training and testing sets.

**Reasoning**:
Divide the dataset into training and testing sets using train_test_split.
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

X=data.loc[:,data.columns!='PCOS (Y/N)'].copy()
y=data['PCOS (Y/N)']

# Identify and one-hot encode the categorical columns
categorical_cols = X.select_dtypes(include=['object']).columns
X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# Impute missing values after one-hot encoding
for col in ['Marraige Status (Yrs)', 'Fast food (Y/N)']:
    if col in X.columns:
        if X[col].isnull().any():
            X[col].fillna(X[col].mode()[0], inplace=True)

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
X_train.shape,X_test.shape,y_train.shape,y_test.shape

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.metrics import classification_report
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis


# Define the models
models = {
    "Logistic Regression": LogisticRegression(solver='liblinear'),
    "Support Vector Machine": SVC(probability=True),
    "Naive Bayes": GaussianNB(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "AdaBoost": AdaBoostClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "MLP Classifier": MLPClassifier(max_iter=1000),
    "Linear Discriminant Analysis": LinearDiscriminantAnalysis()
}

# Train and evaluate each model
for name, model in models.items():
    print(f"--- Training and evaluating {name} ---")
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    print(classification_report(y_test, y_pred))
    print("-" * (len(name) + 30)) # Separator line

# prompt: Create ROC curves

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

plt.figure(figsize=(10, 8))

for name, model in models.items():
    if hasattr(model, "predict_proba"):
        y_prob = model.predict_proba(X_test_scaled)[:, 1]
    elif hasattr(model, "decision_function"):
        y_prob = model.decision_function(X_test_scaled)
    else:
        print(f"Model {name} does not support probability prediction.")
        continue

    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)

    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.legend()
plt.show()

# prompt: Create confusion matrices for each model

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

for name, model in models.items():
    print(f"--- Confusion Matrix for {name} ---")
    y_pred = model.predict(X_test_scaled)
    cm = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f'Confusion Matrix: {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()
    print("-" * (len(name) + 30))

# prompt: Now I want to build an ANN(Artifical Neural Network) for Logistic Regression, 100 Epochs, batch size 42

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming X and y are already loaded and preprocessed as in the previous code

# Scale numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build the ANN model for Logistic Regression
# A single output neuron with a sigmoid activation is used for binary classification (Logistic Regression)
model_ann_lr = keras.Sequential([
    Dense(1, activation='sigmoid', input_shape=(X_train_scaled.shape[1],))
])

# Compile the model
# Using BinaryCrossentropy loss for binary classification and Adam optimizer
model_ann_lr.compile(optimizer='adam',
                     loss='binary_crossentropy',
                     metrics=['accuracy'])

# Train the model
epochs = 100
batch_size = 42

history = model_ann_lr.fit(X_train_scaled, y_train,
                           epochs=epochs,
                           batch_size=batch_size,
                           validation_split=0.2, # Using a validation split during training
                           verbose=1)

# Evaluate the model
loss, accuracy = model_ann_lr.evaluate(X_test_scaled, y_test, verbose=0)
print(f'\nANN Logistic Regression - Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')

# Generate classification report
y_pred_prob = model_ann_lr.predict(X_test_scaled)
y_pred = (y_pred_prob > 0.5).astype(int)
print("\nANN Logistic Regression - Classification Report:")
print(classification_report(y_test, y_pred))

# Plot training history
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Confusion Matrix
cm_ann_lr = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_ann_lr, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix: ANN Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
fpr_ann_lr, tpr_ann_lr, _ = roc_curve(y_test, y_pred_prob)
roc_auc_ann_lr = auc(fpr_ann_lr, tpr_ann_lr)

plt.figure(figsize=(8, 6))
plt.plot(fpr_ann_lr, tpr_ann_lr, color='darkorange', lw=2, label=f'ANN Logistic Regression (AUC = {roc_auc_ann_lr:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve: ANN Logistic Regression')
plt.legend(loc="lower right")
plt.show()

# prompt: Now I want to build an ANN(Artifical Neural Network) for Gradient Boosting, 100 Epochs, batch size 42

import matplotlib.pyplot as plt
# Build the ANN model for Gradient Boosting
# This architecture is a simple feed-forward network
model_ann_gb = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    keras.layers.Dropout(0.3),  # Adding dropout for regularization
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(1, activation='sigmoid') # Output layer for binary classification
])

# Compile the model
# Using BinaryCrossentropy loss and Adam optimizer
model_ann_gb.compile(optimizer='adam',
                     loss='binary_crossentropy',
                     metrics=['accuracy'])

# Define the number of epochs and batch size
epochs = 100
batch_size = 42

# Train the model
print("\n--- Training ANN for Gradient Boosting ---")
history_ann_gb = model_ann_gb.fit(X_train_scaled, y_train,
                                  epochs=epochs,
                                  batch_size=batch_size,
                                  validation_split=0.2, # Using a validation split during training
                                  verbose=1)

# Evaluate the model
loss_ann_gb, accuracy_ann_gb = model_ann_gb.evaluate(X_test_scaled, y_test, verbose=0)
print(f'\nANN Gradient Boosting - Test Loss: {loss_ann_gb:.4f}, Test Accuracy: {accuracy_ann_gb:.4f}')

# Generate classification report
y_pred_prob_ann_gb = model_ann_gb.predict(X_test_scaled)
y_pred_ann_gb = (y_pred_prob_ann_gb > 0.5).astype(int)
print("\nANN Gradient Boosting - Classification Report:")
print(classification_report(y_test, y_pred_ann_gb))

# Plot training history
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history_ann_gb.history['accuracy'], label='Train Accuracy')
plt.plot(history_ann_gb.history['val_accuracy'], label='Validation Accuracy')
plt.title('ANN Gradient Boosting Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history_ann_gb.history['loss'], label='Train Loss')
plt.plot(history_ann_gb.history['val_loss'], label='Validation Loss')
plt.title('ANN Gradient Boosting Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Confusion Matrix
cm_ann_gb = confusion_matrix(y_test, y_pred_ann_gb)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_ann_gb, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix: ANN Gradient Boosting')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
fpr_ann_gb, tpr_ann_gb, _ = roc_curve(y_test, y_pred_prob_ann_gb)
roc_auc_ann_gb = auc(fpr_ann_gb, tpr_ann_gb)

plt.figure(figsize=(8, 6))
plt.plot(fpr_ann_gb, tpr_ann_gb, color='darkorange', lw=2, label=f'ANN Gradient Boosting (AUC = {roc_auc_ann_gb:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve: ANN Gradient Boosting')
plt.legend(loc="lower right")
plt.show()

# prompt: Now I want to build an ANN(Artifical Neural Network) for Random Forest, 100 Epochs, batch size 42

import matplotlib.pyplot as plt
# Build the ANN model for Random Forest
# This architecture is a simple feed-forward network tailored for the dataset
model_ann_rf = keras.Sequential([
    # Input layer and first hidden layer
    keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    keras.layers.Dropout(0.4), # Adding dropout for regularization

    # Second hidden layer
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.4),

    # Third hidden layer
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dropout(0.3),

    # Output layer for binary classification
    keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
# Using BinaryCrossentropy loss and Adam optimizer
model_ann_rf.compile(optimizer='adam',
                     loss='binary_crossentropy',
                     metrics=['accuracy'])

# Define the number of epochs and batch size
epochs = 100
batch_size = 42

# Train the model
print("\n--- Training ANN for Random Forest ---")
history_ann_rf = model_ann_rf.fit(X_train_scaled, y_train,
                                  epochs=epochs,
                                  batch_size=batch_size,
                                  validation_split=0.2, # Using a validation split during training
                                  verbose=1)

# Evaluate the model
loss_ann_rf, accuracy_ann_rf = model_ann_rf.evaluate(X_test_scaled, y_test, verbose=0)
print(f'\nANN Random Forest - Test Loss: {loss_ann_rf:.4f}, Test Accuracy: {accuracy_ann_rf:.4f}')

# Generate classification report
y_pred_prob_ann_rf = model_ann_rf.predict(X_test_scaled)
y_pred_ann_rf = (y_pred_prob_ann_rf > 0.5).astype(int)
print("\nANN Random Forest - Classification Report:")
print(classification_report(y_test, y_pred_ann_rf))

# Plot training history
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history_ann_rf.history['accuracy'], label='Train Accuracy')
plt.plot(history_ann_rf.history['val_accuracy'], label='Validation Accuracy')
plt.title('ANN Random Forest Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history_ann_rf.history['loss'], label='Train Loss')
plt.plot(history_ann_rf.history['val_loss'], label='Validation Loss')
plt.title('ANN Random Forest Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Confusion Matrix
cm_ann_rf = confusion_matrix(y_test, y_pred_ann_rf)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_ann_rf, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix: ANN Random Forest')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
fpr_ann_rf, tpr_ann_rf, _ = roc_curve(y_test, y_pred_prob_ann_rf)
roc_auc_ann_rf = auc(fpr_ann_rf, tpr_ann_rf)

plt.figure(figsize=(8, 6))
plt.plot(fpr_ann_rf, tpr_ann_rf, color='darkorange', lw=2, label=f'ANN Random Forest (AUC = {roc_auc_ann_rf:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve: ANN Random Forest')
plt.legend(loc="lower right")
plt.show()

# prompt: Now I want to build an ANN(Artifical Neural Network) for AdaBoost, 100 Epochs, batch size 42

import matplotlib.pyplot as plt
# Build the ANN model for AdaBoost
# This architecture is a simple feed-forward network tailored for the dataset
model_ann_ab = keras.Sequential([
    # Input layer and first hidden layer
    keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    keras.layers.Dropout(0.4), # Adding dropout for regularization

    # Second hidden layer
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.4),

    # Third hidden layer
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dropout(0.3),

    # Output layer for binary classification
    keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
# Using BinaryCrossentropy loss and Adam optimizer
model_ann_ab.compile(optimizer='adam',
                     loss='binary_crossentropy',
                     metrics=['accuracy'])

# Define the number of epochs and batch size
epochs = 100
batch_size = 42

# Train the model
print("\n--- Training ANN for AdaBoost ---")
history_ann_ab = model_ann_ab.fit(X_train_scaled, y_train,
                                  epochs=epochs,
                                  batch_size=batch_size,
                                  validation_split=0.2, # Using a validation split during training
                                  verbose=1)

# Evaluate the model
loss_ann_ab, accuracy_ann_ab = model_ann_ab.evaluate(X_test_scaled, y_test, verbose=0)
print(f'\nANN AdaBoost - Test Loss: {loss_ann_ab:.4f}, Test Accuracy: {accuracy_ann_ab:.4f}')

# Generate classification report
y_pred_prob_ann_ab = model_ann_ab.predict(X_test_scaled)
y_pred_ann_ab = (y_pred_prob_ann_ab > 0.5).astype(int)
print("\nANN AdaBoost - Classification Report:")
print(classification_report(y_test, y_pred_ann_ab))

# Plot training history
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history_ann_ab.history['accuracy'], label='Train Accuracy')
plt.plot(history_ann_ab.history['val_accuracy'], label='Validation Accuracy')
plt.title('ANN AdaBoost Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history_ann_ab.history['loss'], label='Train Loss')
plt.plot(history_ann_ab.history['val_loss'], label='Validation Loss')
plt.title('ANN AdaBoost Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Confusion Matrix
cm_ann_ab = confusion_matrix(y_test, y_pred_ann_ab)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_ann_ab, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix: ANN AdaBoost')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
fpr_ann_ab, tpr_ann_ab, _ = roc_curve(y_test, y_pred_prob_ann_ab)
roc_auc_ann_ab = auc(fpr_ann_ab, tpr_ann_ab)

plt.figure(figsize=(8, 6))
plt.plot(fpr_ann_ab, tpr_ann_ab, color='darkorange', lw=2, label=f'ANN AdaBoost (AUC = {roc_auc_ann_ab:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve: ANN AdaBoost')
plt.legend(loc="lower right")
plt.show()

# prompt: drop the following: Sl. No, Patient File No., PCOS (Y/N), Hb(g/dl), Cycle(R/I), I   beta-HCG(mIU/mL),	II    beta-HCG(mIU/mL),	FSH(mIU/mL),	LH(mIU/mL)	FSH/LH, TSH (mIU/L),	AMH(ng/mL),	PRL(ng/mL),	Vit D3 (ng/mL),	PRG(ng/mL),	RBS(mg/dl), Follicle No. (L),	Follicle No. (R),	Avg. F size (L) (mm),	Avg. F size (R) (mm),	Endometrium (mm)

columns_to_drop = [
    "Sl. No",
    "Patient File No.",
    "PCOS (Y/N)",
    "Hb(g/dl)",
    "Cycle(R/I)",
    "I   beta-HCG(mIU/mL)",
    "II    beta-HCG(mIU/mL)",
    "FSH(mIU/mL)",
    "LH(mIU/mL)",
    "FSH/LH",
    "TSH (mIU/L)",
    "AMH(ng/mL)",
    "PRL(ng/mL)",
    "Vit D3 (ng/mL)",
    "PRG(ng/mL)",
    "RBS(mg/dl)",
    "Follicle No. (L)",
    "Follicle No. (R)",
    "Avg. F size (L) (mm)",
    "Avg. F size (R) (mm)",
    "Endometrium (mm)",
]

# Drop the specified columns from the original data
# Ensure column names are stripped of leading/trailing whitespace if necessary
data.columns = data.columns.str.strip()
columns_to_drop = [col.strip() for col in columns_to_drop] # Strip whitespace from the list too

# Check if columns exist before dropping to avoid errors
cols_to_drop_exist = [col for col in columns_to_drop if col in data.columns]
if cols_to_drop_exist:
    data = data.drop(cols_to_drop_exist, axis=1)
    print(f"Dropped columns: {cols_to_drop_exist}")
else:
    print("None of the specified columns to drop were found in the dataframe.")

data.head()

pip install streamlit

# Execute the cell that defines model_ann_rf (cell id: 6fjVg83RXudy)
# This code block is for demonstration and cannot directly execute another cell in Colab.
# In a real scenario, you would manually run the cell above or ensure its execution
# before the cell that saves the model.

# For the purpose of demonstration within this response, I will include the model definition here
# and then the saving code. In your notebook, you should ensure the cell 6fjVg83RXudy is run first.

# --- Start of code from cell 6fjVg83RXudy (for demonstration) ---
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc

# Assuming X and y are already loaded and preprocessed as in the previous code

# Scale numerical features
# Since X_train and X_test are already scaled in the notebook, we can use them directly
# scaler = StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_test_scaled = scaler.transform(X_test)

# Build the ANN model for Random Forest (as requested, though the architecture is a general ANN)
model_ann_rf = keras.Sequential([
    # Input layer and first hidden layer
    keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    keras.layers.Dropout(0.4), # Adding dropout for regularization

    # Second hidden layer
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.4),

    # Third hidden layer
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dropout(0.3),

    # Output layer for binary classification
    keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model_ann_rf.compile(optimizer='adam',
                     loss='binary_crossentropy',
                     metrics=['accuracy'])

# Define the number of epochs and batch size
epochs = 100
batch_size = 42

# Train the model
print("\n--- Training ANN for Random Forest ---")
history_ann_rf = model_ann_rf.fit(X_train_scaled, y_train,
                                  epochs=epochs,
                                  batch_size=batch_size,
                                  validation_split=0.2, # Using a validation split during training
                                  verbose=1)

# Evaluate the model (optional, as it's already done in the original cell)
# loss_ann_rf, accuracy_ann_rf = model_ann_rf.evaluate(X_test_scaled, y_test, verbose=0)
# print(f'\nANN Random Forest - Test Loss: {loss_ann_rf:.4f}, Test Accuracy: {accuracy_ann_rf:.4f}')

# Generate classification report (optional)
# y_pred_prob_ann_rf = model_ann_rf.predict(X_test_scaled)
# y_pred_ann_rf = (y_pred_prob_ann_rf > 0.5).astype(int)
# print("\nANN Random Forest - Classification Report:")
# print(classification_report(y_test, y_pred_ann_rf))

# Plot training history (optional)
# plt.figure(figsize=(12, 4))
# plt.subplot(1, 2, 1)
# plt.plot(history_ann_rf.history['accuracy'], label='Train Accuracy')
# plt.plot(history_ann_rf.history['val_accuracy'], label='Validation Accuracy')
# plt.title('ANN Random Forest Model Accuracy')
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.legend()
#
# plt.subplot(1, 2, 2)
# plt.plot(history_ann_rf.history['loss'], label='Train Loss')
# plt.plot(history_ann_rf.history['val_loss'], label='Validation Loss')
# plt.title('ANN Random Forest Model Loss')
# plt.xlabel('Epoch')
# plt.ylabel('Loss')
# plt.legend()
# plt.show()

# Confusion Matrix (optional)
# cm_ann_rf = confusion_matrix(y_test, y_pred_ann_rf)
# plt.figure(figsize=(6, 4))
# sns.heatmap(cm_ann_rf, annot=True, fmt='d', cmap='Blues', cbar=False)
# plt.title('Confusion Matrix: ANN Random Forest')
# plt.xlabel('Predicted')
# plt.ylabel('Actual')
# plt.show()

# ROC Curve (optional)
# fpr_ann_rf, tpr_ann_rf, _ = roc_curve(y_test, y_pred_prob_ann_rf)
# roc_auc_ann_rf = auc(fpr_ann_rf, tpr_ann_rf)
#
# plt.figure(figsize=(8, 6))
# plt.plot(fpr_ann_rf, tpr_ann_rf, color='darkorange', lw=2, label=f'ANN Random Forest (AUC = {roc_auc_ann_rf:.2f})')
# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.title('ROC Curve: ANN Random Forest')
# plt.legend(loc="lower right")
# plt.show()

# --- End of code from cell 6fjVg83RXudy ---

# Now, save the trained model
import pickle

# Define the file path to save the model in Google Drive
model_filename = '/content/drive/MyDrive/PCOS_ANN_RandomForest_model.pkl'

# Save the trained model using pickle
with open(model_filename, 'wb') as f:
    pickle.dump(model_ann_rf, f)

print(f"Trained model saved successfully to {model_filename}")

# prompt: Save the trained model in a format suitable for deployment (e.g., using pickle in Python).

import pickle

# Assuming 'model_ann_rf' is the trained model you want to save
# and it's the best performing one based on your evaluations.

# Define the file path to save the model in Google Drive
model_filename = '/content/drive/MyDrive/PCOS_ANN_RandomForest_model.pkl'

# Save the trained model using pickle
with open(model_filename, 'wb') as f:
    pickle.dump(model_ann_rf, f)

print(f"Trained model saved successfully to {model_filename}")

# To load the model later for prediction:
# with open(model_filename, 'rb') as f:
#     loaded_model = pickle.load(f)
#
# print("Model loaded successfully.")
#
# # Example of how to use the loaded model for prediction:
# # Assuming you have new data X_new_scaled
# # predictions = loaded_model.predict(X_new_scaled)
# # print(predictions)

"""Convert ipynb file to py file from CLI.
Usage:
    python ./convert_ipynb_to_py.py [full input path] [full output path]
!! overwrites output path without warning if it exists !!
"""
import sys
import json


with open(sys.argv[1], "r") as f:  # input.ipynb
    j = json.load(f)
with open(sys.argv[2], "w") as of:  # output.py
    cells = j["cells"] if j["nbformat"] >= 4 else j["worksheets"][0]["cells"]
    for i, cell in enumerate(cells):
        of.write("\n# cell " + str(i) + "\n")
        for line in cell["source"]:
            of.write(line)
        of.write("\n\n")
